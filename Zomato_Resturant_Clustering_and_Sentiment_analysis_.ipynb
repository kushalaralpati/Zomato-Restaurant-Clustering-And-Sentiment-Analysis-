{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "Yfr_Vlr8HBkt"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kushalaralpati/Zomato-Restaurant-Clustering-And-Sentiment-Analysis-/blob/main/Zomato_Resturant_Clustering_and_Sentiment_analysis_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - Unsupervised ML - Zomato Restaurant Clustering And Sentiment Analysis\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Team Member**     - Kushala R S"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This project is centered on analyzing restaurant data in India to extract insights regarding cuisines, costs, ratings, collections, and sentiments. The methodology encompasses data wrangling, dimensionality reduction, and the application of machine learning models such as K-means and hierarchical clustering. Additionally, topic modeling using LDA and sentiment analysis techniques are utilized. The resulting findings offer valuable information on prevalent cuisines, restaurant affordability, rating trends, and customer sentiments. The report includes recommendations and suggestions for future work to encourage further exploration in this domain.\n",
        "\n",
        "Data Preprocessing:\n",
        "\n",
        "Dataset Acquisition: Obtained the Zomato restaurant dataset for analysis.\n",
        "\n",
        "Library Integration: Imported necessary libraries for data manipulation and analysis.\n",
        "\n",
        "Data Import: Loaded the Zomato dataset into the analysis environment.\n",
        "\n",
        "Missing Data Handling: Addressed any missing values in the dataset.\n",
        "\n",
        "Categorical Data Encoding: Converted categorical data into numerical format for machine learning compatibility.\n",
        "\n",
        "Data Cleaning and Feature Engineering: Performed data cleaning and created new features to enhance the dataset's quality.\n",
        "\n",
        "Exploratory Data Analysis (EDA):\n",
        "\n",
        "Top 5 Cuisines: Identified the top 5 cuisines in the dataset using a pie chart.\n",
        "\n",
        "Restaurant Cost Distribution: Visualized the distribution of restaurants based on their cost using a histogram or bar chart.\n",
        "\n",
        "Most Expensive and Affordable Restaurants: Identified the most expensive and most affordable restaurants in the dataset.\n",
        "\n",
        "Frequency Distribution of Collections: Analyzed the frequency distribution of restaurant collections (types) to understand popular categories.\n",
        "\n",
        "Overall Distribution of Restaurant Ratings: Plotted a histogram to visualize the distribution of restaurant ratings across the dataset.\n",
        "\n",
        "Rating Variation over Hours: Created a Plotly barchart to explore how restaurant ratings change over different hours of the day.\n",
        "\n",
        "Clustering Result Visualization: Utilized scatter plots to visualize the clustering result obtained from K-means or hierarchical clustering.\n",
        "\n",
        "Dendrogram: Generated a dendrogram to display the hierarchy of clusters in hierarchical clustering.\n",
        "\n",
        "Machine Learning Model Implementation:\n",
        "\n",
        "K-means: Used K-means algorithm for clustering restaurants based on similarities.\n",
        "\n",
        "Silhouette Score Graph: Plotted a graph to determine the optimal number of clusters using the silhouette score.\n",
        "\n",
        "Hierarchical Clustering: Employed hierarchical clustering and created a dendrogram to visualize the hierarchy of clusters.\n",
        "\n",
        "Topic Modeling using LDA:\n",
        "\n",
        "Performed topic modeling using Latent Dirichlet Allocation (LDA) on restaurant reviews to identify key topics and themes.\n",
        "\n",
        "Sentiment Analysis:\n",
        "\n",
        "Conducted sentiment analysis on restaurant reviews to determine positive, negative, and neutral sentiments, gaining insights into customer preferences and satisfaction levels.\n",
        "\n",
        "Conclusion:\n",
        "\n",
        "The Zomato Restaurant Clustering and Sentiment Analysis project effectively conducted exploratory data analysis (EDA) on the dataset, unearthing significant insights about the leading cuisines, distribution of restaurant costs, and prevalent restaurant categories. Leveraging clustering techniques such as K-means and hierarchical clustering facilitated the grouping of similar restaurants, enabling a better understanding of restaurant similarities. Furthermore, topic modeling and sentiment analysis delved deeper into customer reviews and preferences, providing invaluable insights. The project's findings have the potential to empower restaurant owners and stakeholders to make informed decisions and enhance their services, thereby effectively catering to customer needs."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Zomato is an Indian restaurant aggregator and food delivery start-up founded by Deepinder Goyal and Pankaj Chaddah in 2008. Zomato provides information, menus and user-reviews of restaurants, and also has food delivery options from partner restaurants in select cities. India is quite famous for its diverse multi cuisine available in a large number of restaurants and hotel resorts, which is reminiscent of unity in diversity. Restaurant business in India is always evolving. More Indians are warming up to the idea of eating restaurant food whether by dining outside or getting food delivered. The growing number of restaurants in every state of India has been a motivation to inspect the data to get some insights, interesting facts and figures about the Indian food industry in each city. So, this project focuses on analysing the Zomato restaurant data for each city in India.\n",
        "\n",
        "The Project focuses on Customers and Company, you have to analyze the sentiments of the reviews given by the customer in the data and make some useful conclusions in the form of Visualizations. Also, cluster the zomato restaurants into different segments. The data is vizualized as it becomes easy to analyse data at instant. The Analysis also solves some of the business cases that can directly help the customers finding the Best restaurant in their locality and for the company to grow up and work on the fields they are currently lagging in. This could help in clustering the restaurants into segments. Also the data has valuable information around cuisine and costing which can be used in cost vs. benefit analysis Data could be used for sentiment analysis. Also the metadata of reviewers can be used for identifying the critics in the industry.\n",
        "\n",
        "What are the top 5 cuisines based on the pie chart?\n",
        "\n",
        "How is the distribution of restaurants by cost?\n",
        "\n",
        "Which are the most expensive and most affordable restaurants?\n",
        "\n",
        "What is the frequency distribution of collections?\n",
        "\n",
        "How are restaurant ratings distributed overall?\n",
        "\n",
        "What does the Plotly barchart reveal about ratings over the hour?\n",
        "\n",
        "How is the heatmap used in the data analysis?\n",
        "\n",
        "How were outliers handled using IQR?\n",
        "\n",
        "What are the key steps in textual data preprocessing?\n",
        "\n",
        "How was Dimensionality reduction achieved using PCA?"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "from matplotlib.colors import ListedColormap\n",
        "import matplotlib.cm as cm\n",
        "import seaborn as sns\n",
        "import time\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "from scipy.stats import norm\n",
        "from scipy import stats\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "\n",
        "#importing kmeans\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "#Non-negative matrix Factorization\n",
        "from sklearn.decomposition import NMF\n",
        "\n",
        "#principal component analysis\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "#silhouette score\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "#importing stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "#for tokenization\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# for POS tagging(Part of speech in NLP sentiment analysis)\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# for sentiment analysis\n",
        "from textblob import TextBlob\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "#import stemmer\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "#import tfidf\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "df1 = pd.read_csv('/content/Zomato Restaurant names and Metadata.csv')\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.read_csv('/content/Zomato Restaurant reviews.csv')"
      ],
      "metadata": {
        "id": "o5h0BXYooVto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df1.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Last  Look\n",
        "df1.tail()"
      ],
      "metadata": {
        "id": "zBArZOgT94vF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset first look of Review\n",
        "df2.head()"
      ],
      "metadata": {
        "id": "lViAkvES-MaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df1.columns"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.columns"
      ],
      "metadata": {
        "id": "wiZ4_Fjo_xhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df1.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.info()"
      ],
      "metadata": {
        "id": "0H80VGjAAHaG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df1.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.duplicated().sum()"
      ],
      "metadata": {
        "id": "gvGzLu4GAVe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df1.isnull().sum()"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.isnull().sum()"
      ],
      "metadata": {
        "id": "xtBlB-bKAgZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "#to visualize the data we can use the heatmap\n",
        "sns.set_theme()\n",
        "sns.set(rc={\"figure.dpi\":120, \"figure.figsize\":(8,6)})\n",
        "sns.heatmap(df1.isnull(), cbar=False)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's Visualize the missing values\n",
        "# to visualize missing data we can use the heatmap.\n",
        "\n",
        "sns.set_theme()\n",
        "sns.set(rc={\"figure.dpi\":120, \"figure.figsize\":(8,6)})\n",
        "sns.heatmap(df2.isnull(), cbar=False)"
      ],
      "metadata": {
        "id": "3uN4rA7gA4VN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restaurant DataSet:\n",
        "\n",
        "105 observations with 6 features.\n",
        "\n",
        "\"Collection\" and \"Timing\" features have null values.\n",
        "\n",
        "No duplicate values, all 105 data points are unique.\n",
        "\n",
        "\"Cost\" feature represents amounts but has an object data type due to comma separation.\n",
        "\n",
        "\"Timing\" represents operational hours as text with an object data type.\n",
        "\n",
        "Review DataSet:\n",
        "\n",
        "10000 observations with 7 features.\n",
        "\n",
        "Except \"Picture\" and \"Restaurant\" features, all others have null values.\n",
        "\n",
        "There are 36 duplicate values for two restaurants - \"American Wild Wings\" and \"Arena Eleven,\" where these duplicates mostly have null values.\n",
        "\n",
        "\"Rating\" represents ordinal data but has an object data type; it should be converted to an integer.\n",
        "\n",
        "\"Timing\" represents the time when reviews were posted as object data; it should be converted into date-time format."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Descrfibe\n",
        "df1.describe(include='all').T\n"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe Review\n",
        "df1.describe(include='all').T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variables Description\n",
        "\n",
        " - Name of Restaurants\n",
        " - URL Links of Restaurants\n",
        " - Per Person estimated cost of dining\n",
        " - Tagging of Restaurant with respect to Zomato categories\n",
        " - Cuisines served by restaurants\n",
        " - Restaurant timings\n",
        "\n",
        "Dataset 2\n",
        "\n",
        "Restuarant: Name of the restaurants being reviewed.\n",
        "\n",
        "Reviewer: Name of the reviewer providing the review.\n",
        "\n",
        "Review: The text of the review.\n",
        "\n",
        "Rating: The rating provided by the reviewer.\n",
        "\n",
        "MetaData: Information about the reviewer, such as the number of reviews they have given and the number of followers they have.\n",
        "\n",
        "Time: Date and time of the review.\n",
        "\n",
        "Pictures: Number of pictures posted along with the review."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "# Let's Check Unique Values  of df1 for each variable\n",
        "\n",
        "for ele in df1:\n",
        "  print(f'Number of unique values in {ele} is {df1[ele].nunique()}.')"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's Check Unique Values of review for each variable.\n",
        "\n",
        "for ele in df2:\n",
        "  print(f'Number of unique values in {ele} is {df2[ele].nunique()}.')"
      ],
      "metadata": {
        "id": "DZ9j1-UauMqi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's remove duplicate value from df2 Dataframe.\n",
        "df2.drop_duplicates(inplace=True)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1G18GnpswxMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count of Review\n",
        "df2.isnull().sum()"
      ],
      "metadata": {
        "id": "mPRzZzatw1VO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's explore null values in review coloumn\n",
        "df2[df2[\"Review\"].isnull()]"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop rows where the \"Review\" column is null\n",
        "df2.dropna(subset=[\"Review\"], inplace=True)"
      ],
      "metadata": {
        "id": "NuGLiusWxXJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# changing the data type of the cost function of Restaurant\n",
        "df1['Cost'] = df1['Cost'].str.replace(\",\",\"\").astype('int64')"
      ],
      "metadata": {
        "id": "6fpxdvjexabi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's Extract Year, Month, Day and Hour from Column Time.\n",
        "df2['Time']=pd.to_datetime(df2['Time'])\n",
        "df2['Year']=df2['Time'].dt.year\n",
        "df2['Month']=df2['Time'].dt.month\n",
        "df2['Day']=df2['Time'].dt.day_name()\n",
        "df2['Hour']=df2['Time'].dt.hour"
      ],
      "metadata": {
        "id": "e6f13CC1xdNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# substitutiong 'like' observation first by nan values ans then by mean of the feature\n",
        "df2.loc[df2['Rating'] == 'Like'] = np.nan\n",
        "df2['Rating'] = df2['Rating'].astype('float64')\n",
        "print(df2['Rating'].mean())"
      ],
      "metadata": {
        "id": "s6PRwpR3xfyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing all the cusines of Restaurant\n",
        "cuisine_value_list = df1['Cuisines'].str.split(', ').apply(lambda x: [cuisine.strip() for cuisine in x])"
      ],
      "metadata": {
        "id": "Fd8a5UeSxzXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting the dict to a data frame\n",
        "cuisine_counter = Counter([cuisine for cuisine_names in cuisine_value_list for cuisine in cuisine_names])\n",
        "cuisine_df = pd.DataFrame.from_dict(cuisine_counter, orient='index', columns=['Number of Restaurants']).reset_index().rename(columns={'index': 'Cuisine'})\n",
        "top_5_cuisines = cuisine_df.sort_values('Number of Restaurants', ascending=False).head(5)\n"
      ],
      "metadata": {
        "id": "_LjzUK01x1N_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's print top cuisines\n",
        "print(top_5_cuisines)"
      ],
      "metadata": {
        "id": "zsIpi4uIx3OL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-naming in dataframe \"Name\" to \"Restaurant\"\n",
        "df1 = df1.rename(columns = {'Name':'Restaurant'})"
      ],
      "metadata": {
        "id": "Qszl-BITyOg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Duplicate values from the Review dataframe were dropped to ensure data integrity.\n",
        "\n",
        "Null values in the review dataframe were removed to clean the data and avoid any potential biases.\n",
        "\n",
        "The data type of the 'cost' function was converted from string to integer to facilitate numerical analysis.\n",
        "\n",
        "The Year, Month, Day, and Hour components were extracted from the 'Time' column to enable temporal analysis.\n",
        "\n",
        "The 'like' observations were first substituted with NaN values and then replaced with the mean of the feature to handle missing data appropriately.\n",
        "\n",
        "A separate cuisine dataframe was created to analyze the various cuisines served by the restaurants."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "# Let's plot a pie chart for the top 5 cuisines\n",
        "\n",
        "# Data for the pie chart\n",
        "top_5_cuisines = {\n",
        "    'Cuisine': ['North Indian', 'Chinese', 'Continental', 'Biryani', 'Fast Food'],\n",
        "    'Number of Restaurants': [61, 43, 21, 16, 15]\n",
        "}\n",
        "\n",
        "# Create the pie chart\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(top_5_cuisines['Number of Restaurants'], labels=top_5_cuisines['Cuisine'], autopct='%.0f%%', startangle=90, shadow=True)\n",
        "\n",
        "# Set title and aspect ratio\n",
        "plt.title('Top 5 Most Selling Cuisines')\n",
        "plt.axis('equal')\n",
        "\n",
        "# Show the pie chart\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is useful for identifying the distribution of data across categories at a glance."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "North indian and chinese cuisines are high selling food"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Let's find what is distribution of restaurants based on cost\n",
        "# Let's Define the cost categories and corresponding labels\n",
        "cost_categories = {\n",
        "    'low': (0, 500),\n",
        "    'medium': (500, 1000),\n",
        "    'high': (1000, 2000),\n",
        "    'very high': (2000, float('inf'))\n",
        "}\n",
        "\n",
        "# Categorize df1s based on their cost\n",
        "def categorize_cost(cost):\n",
        "    for category, (min_cost, max_cost) in cost_categories.items():\n",
        "        if min_cost <= cost <= max_cost:\n",
        "            return category\n",
        "\n",
        "# Apply the categorization function to the 'Cost' column\n",
        "df1['Cost Category'] = df1['Cost'].apply(categorize_cost)\n",
        "\n",
        "# Count the number of df1s in each cost category\n",
        "cost_category_counts = df1['Cost Category'].value_counts()\n",
        "plt.figure(figsize=(8, 8))\n",
        "# Create a pie chart\n",
        "explode = (0, 0.025, 0.05, 0.01)  # Explode the 'medium' slice\n",
        "plt.pie(cost_category_counts, labels=cost_category_counts.index, autopct='%1.1f%%', colors=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'], explode=explode)\n",
        "\n",
        "# Set the title\n",
        "plt.title('Distribution of Restaurants by Cost', fontsize=18)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is useful for identifying the distribution of data across categories at a glance."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Majority are in medium and low categories, while only a small percentage are very high. Valuable insights for business decisions and marketing."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# Let's check for most expensive Restaurant\n",
        "sns.barplot(x='Cost',\n",
        "            y=\"Restaurant\",\n",
        "            data=df1,\n",
        "            order=df1.sort_values('Cost', ascending=False).Restaurant[:15])\n",
        "\n",
        "plt.title('15 Most Expensive Restaurants', fontsize=18)\n",
        "plt.xlabel('Cost-------->', fontsize=15)\n",
        "plt.ylabel('Name of Restaurants', fontsize=15)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar chart is useful to comparing data across categories of groups"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collage- Hyatt Hyderabad Gachibowli Restaurant is so expensive followed by Feast- sheraton Hyderbad Hotel"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# checking for most affordable Restaurant\n",
        "sns.barplot(x='Cost',\n",
        "            y=\"Restaurant\",\n",
        "            data=df1,\n",
        "            order=df1.sort_values('Cost',ascending=False).Restaurant[-15:])\n",
        "\n",
        "plt.title('15 Most affordable Restaurant', fontsize=18)\n",
        "plt.xlabel('Cost------->', fontsize=15)\n",
        "plt.ylabel('Name of Restaurants', fontsize=15)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bar char is useful to comparing data across categories of groups of restaurants."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I found that Amul is the restaurant at lowest cost that is 150 followed by Mohammedia Shawarma."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# Creating word cloud for collection\n",
        "plt.figure(figsize=(15,8))\n",
        "text = \" \".join(name for name in df1.Cuisines )\n",
        "\n",
        "# Creating word_cloud with text as argument in .generate() method\n",
        "\n",
        "word_cloud = WordCloud(width = 1400, height = 1400,collocations = False, background_color = 'black').generate(text)\n",
        "\n",
        "# Display the generated Word Cloud\n",
        "\n",
        "plt.imshow(word_cloud, interpolation='bilinear')\n",
        "\n",
        "plt.axis(\"off\")"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word cloud is an essential data visualization tool that concisely displays word frequencies."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word cloud analysis reveals North Indian, Chinese, Continental, and fast food as the most preferred restaurant cuisines."
      ],
      "metadata": {
        "id": "iCGJZlBW-6K3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "# Creating word cloud for expensive restaurants.\n",
        "plt.figure(figsize=(15,8))\n",
        "text = \" \".join(name for name in df1.sort_values('Cost',ascending=False).Restaurant[:30])\n",
        "\n",
        "# Creating word_cloud with text as argument in .generate() method.\n",
        "word_cloud = WordCloud(width = 1400, height = 1400,collocations = False, background_color = 'black').generate(text)\n",
        "\n",
        "# Display the generated Word Cloud.\n",
        "plt.imshow(word_cloud, interpolation='bilinear')\n",
        "\n",
        "plt.axis(\"off\");"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word cloud is helpfull for identify prominent themes, patterns, or trends within a text or dataset."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The word cloud analysis of restaurant names highlights frequent occurrences of \"Hyderabad,\" \"Sheraton,\" \"Suites,\" and \"Holiday.\""
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# Let's have look on frequency distribution of Collections.\n",
        "df1['Collections'].value_counts()[0:10].sort_values().plot(kind='barh')\n",
        "plt.xlabel('Collection')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Value Counts of Collection')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Value count bar charts visually display the frequency distribution of categorical data, providing quick insights into data patterns and helping identify dominant categories."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most frequent Collection is Food Hygiene Rated Restaurants in Hyderabad followed by others collect like Trending this week, veggie friendly so on."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# Let's see overall distribution of restaurant ratings\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.distplot(df2['Rating'], bins=10, kde=True, rug=True)\n",
        "plt.xlabel('Rating',fontsize=12)\n",
        "plt.ylabel('Frequency',fontsize=12)\n",
        "plt.title('Overall Distribution of Restaurant Ratings',fontsize=14)\n",
        "plt.xticks(range(1, 6))  # Set x-axis ticks to range from 1 to 5 (ratings scale)\n",
        "plt.grid(axis='y')  # Add gridlines to the y-axis for better readability\n",
        "\n",
        "# Calculate mean and median\n",
        "mean_rating = df2['Rating'].mean()\n",
        "median_rating = df2['Rating'].median()\n",
        "\n",
        "# Plot mean line\n",
        "plt.axvline(mean_rating, color='red', linestyle='--', label=f'Mean Rating: {mean_rating:.2f}')\n",
        "\n",
        "# Plot median line\n",
        "plt.axvline(median_rating, color='green', linestyle='-.', label=f'Median Rating: {median_rating:.2f}')\n",
        "\n",
        "plt.legend()  # Show legend with mean and median labels\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distplot is useful for visually understanding the distribution of a numerical variable, providing insights into the data's central tendency, spread, and overall shape."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The majority of ratings are high (around 5.0), with fewer extreme ratings, suggesting overall positive feedback and few very low ratings."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "# Let's plot a graph to understand Review distribution\n",
        "days_count = df2['Day'].value_counts()\n",
        "total_days = len(df2['Day'])\n",
        "percentage_days = days_count / total_days * 100\n",
        "\n",
        "# Create a donut chart\n",
        "fig = go.Figure(data=[go.Pie(\n",
        "    labels=days_count.index,\n",
        "    values=percentage_days,\n",
        "    hole=0.5,\n",
        "    textinfo='percent',\n",
        "    marker=dict(colors=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2'])\n",
        ")])\n",
        "\n",
        "# Set the title and legend color\n",
        "title_color = '#8a8d93'\n",
        "legend_color = 'white'\n",
        "\n",
        "fig.update_layout(\n",
        "    title='Reviews Distribution by Day',\n",
        "    title_font=dict(size=25, color=title_color, family=\"Lato, sans-serif\"),\n",
        "    plot_bgcolor='#444',\n",
        "    paper_bgcolor='#444',\n",
        "    legend=dict(\n",
        "        bgcolor='rgba(0,0,0,0)',  # Set the legend background color to transparent\n",
        "        font=dict(color=legend_color)\n",
        "    )\n",
        ")\n",
        "\n",
        "# Display the chart\n",
        "fig.show(renderer='colab')\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotly donut are useful charts due to its interactive and dynamic capabilities, allowing users to explore data."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Donut chart shows peak outing on weekends (Sunday & Saturday), with significant activity on Friday and Wednesday."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Hypothesis Test 1**: Is there a significant relationship between the cost of a restaurant and the rating it receives?\n",
        "\n",
        " **Null Hypothesis (H0)**:\n",
        "There is no significant relationship between the cost of a restaurant and the rating it receives.\n",
        "\n",
        "**Alternative Hypothesis (H1)**:\n",
        "There is a significant relationship between the cost of a restaurant and the rating it receives."
      ],
      "metadata": {
        "id": "LhOspSrsb0q1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's Merged Restaurant and Review DataFrame on Restaurant column\n",
        "merged_df = df1.merge(df2, on = 'Restaurant')\n",
        "merged_df.shape\n"
      ],
      "metadata": {
        "id": "s9uYlhE2c6u9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Library\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Step 1: Prepare the data\n",
        "if 'Rating' not in merged_df.columns or 'Cost' not in merged_df.columns:\n",
        "    raise ValueError(\"The 'Rating' and 'Cost' columns are required in the 'merged_df' DataFrame.\")\n",
        "\n",
        "# Drop rows with missing values in 'Rating' or 'Cost' (if necessary)\n",
        "merged_df.dropna(subset=['Rating', 'Cost'], inplace=True)\n",
        "\n",
        "# Step 2: Fit the linear regression model\n",
        "X = merged_df['Cost']   # Independent variable (Cost)\n",
        "y = merged_df['Rating'] # Dependent variable (Ratings)\n",
        "\n",
        "# Add a constant term to the independent variable (required for statsmodels)\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Fit the linear regression model\n",
        "model = sm.OLS(y, X).fit()\n",
        "\n",
        "# Step 3: Check p-value of the coefficient for 'Cost'\n",
        "p_value = model.pvalues['Cost']\n",
        "\n",
        "# Step 4: Set the significance level (alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Step 5: Perform the hypothesis test\n",
        "if p_value < alpha:\n",
        "    print(\"Reject Null Hypothesis: There is a significant relationship between the cost of a restaurant and the rating it receives.\")\n",
        "else:\n",
        "    print(\"Fail to reject Null Hypothesis: There is no significant relationship between the cost of a restaurant and the rating it receives.\")\n",
        "\n",
        "# Step 6: Interpret the results\n",
        "# Based on the test result, provide a clear interpretation of whether the cost of a restaurant has a significant impact on the ratings it receives.\n",
        "\n",
        "# Optional: Print the regression summary for additional insights\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "RCpCeMWVc-bG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothesis Test 2**: Do restaurants with more pictures receive higher ratings?\n",
        "\n",
        " **Null Hypothesis(H0)**: There is no significant relationship between the number of pictures and the rating of the restaurant.\n",
        "\n",
        "** Alternative Test(H1)**: There is a significant relationship between the number of pictures and the rating of the restaurant.\n",
        "\n",
        "Test: Perform a Pearson correlation test between the 'Pictures' and 'Rating' columns."
      ],
      "metadata": {
        "id": "FOwS1-dGejO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Step 1: Prepare the data\n",
        "# if 'Rating' not in merged_df.columns or 'Pictures' not in merged_df.columns:\n",
        "#     raise ValueError(\"The 'Rating' and 'Pictures' columns are required in the 'merged_df' DataFrame.\")\n",
        "\n",
        "# # Drop rows with missing values in 'Rating' or 'Pictures' (if necessary)\n",
        "# merged_df.dropna(subset=['Rating', 'Pictures'], inplace=True)\n",
        "\n",
        "# Step 2: Fit the linear regression model\n",
        "X = merged_df['Pictures'] # Independent variable (Number of pictures)\n",
        "y = merged_df['Rating']   # Dependent variable (Ratings)\n",
        "\n",
        "# Add a constant term to the independent variable (required for statsmodels)\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# Fit the linear regression model\n",
        "model = sm.OLS(y, X).fit()\n",
        "\n",
        "# Step 3: Check p-value of the coefficient for 'Pictures'\n",
        "p_value = model.pvalues['Pictures']\n",
        "\n",
        "# Step 4: Set the significance level (alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Step 5: Perform the hypothesis test\n",
        "if p_value < alpha:\n",
        "    print(\"Reject Null Hypothesis: There is a significant relationship between the number of pictures and the ratings of the restaurants.\")\n",
        "else:\n",
        "    print(\"Fail to reject Null Hypothesis: There is no significant relationship between the number of pictures and the ratings of the restaurants.\")\n",
        "\n",
        "# Step 6: Interpret the results\n",
        "# Based on the test result, provide a clear interpretation of whether the number of pictures has a significant impact on the ratings of the restaurants.\n",
        "\n",
        "# Optional: Print the regression summary for additional insights\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "id": "cSGOHbwH_by1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which statistical test have you done to obtain P-Value?\n",
        "\n",
        "I used a linear regression model to obtain the P-value. Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables .\n",
        "\n"
      ],
      "metadata": {
        "id": "eprI_rVo_iR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why did you choose the specific statistical test?\n",
        "\n",
        "linear regression model is a widely-used and versatile statistical test for examining the relationship between two continuous variables and has the advantage of providing both statistical significance testing and practical interpretability of the results."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "source": [
        "#changing date and extracting few feature for manipulation\n",
        "df2[['Reviewer_Total_Review', 'Reviewer_Followers']] = df2['Metadata'].str.split(',', n=1, expand=True)\n",
        "df2['Reviewer_Total_Review'] = pd.to_numeric(df2['Reviewer_Total_Review'].str.split(' ').str[0])\n",
        "df2['Reviewer_Followers'] = pd.to_numeric(df2['Reviewer_Followers'].str.split(' ').str[1])"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "WSY45QfZ2wRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Price point of restaurants\n",
        "price_point = merged_df.groupby('Restaurant').agg({'Rating':'mean',\n",
        "        'Cost': 'mean'}).reset_index().rename(columns = {'Cost': 'Price_Point'})"
      ],
      "metadata": {
        "id": "igiVKlq_AFXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# price point for high rated restaurants\n",
        "price_point.nlargest(5,'Rating')"
      ],
      "metadata": {
        "id": "E224014nAHzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Info of merged data\n",
        "merged_df.info()"
      ],
      "metadata": {
        "id": "9swKFxjGAN91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's Convert the \"Followers\" column to integer type and fill null values with zero\n",
        "df2['Reviewer_Followers'] = df2['Reviewer_Followers'].fillna(0).astype(int)\n",
        "df2['Reviewer_Total_Review'] = df2['Reviewer_Total_Review'].fillna(0).astype(int)"
      ],
      "metadata": {
        "id": "XYOln4f4AQs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Let's plot a heatmap visualizes correlation strength between variables\n",
        "f, ax = plt.subplots(figsize=(15, 10))\n",
        "\n",
        "# Calculate correlations on numerical columns only\n",
        "numerical_df = merged_df.select_dtypes(include=['number'])\n",
        "\n",
        "# Generate the heatmap using the correlation matrix of the numerical data\n",
        "sns.heatmap(numerical_df.corr(), ax=ax, annot=True, cmap='coolwarm', linewidths=1)\n",
        "\n",
        "# Show the heatmap\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "xHR0UfKp3OpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?\n",
        "\n",
        "Heatmap: Visualizes correlation strength between variables. Darker colors show strong relationships, while lighter colors indicate weaker connections. Provides valuable insights for informed decision-making.\n",
        "\n"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is/are the insight(s) found from the chart?\n",
        "\n",
        "\n",
        "1. Cost-Rating: Weak positive (0.144) - Higher cost, slightly higher ratings.\n",
        "\n",
        "2. Pictures-Reviewer_Total_Review: Moderate positive (0.331) - More pictures, more reviews.\n",
        "3. Year-Month: Strong negative (-0.758) - As year increases, month decreases (chronological).\n",
        "4. Cost-Year: Moderate positive (0.250) - Recent years, higher cost.\n",
        "5. Year-Reviewer_Total_Review: Weak negative (-0.056) - More reviews, earlier years.\n",
        "6. Reviewer_Total_Review-Reviewer_Followers: Strong positive (0.455) - More reviews, more followers."
      ],
      "metadata": {
        "id": "V5h_LKORAh1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Select numerical columns only (excluding non-numeric columns)\n",
        "numerical_columns = merged_df.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "# Loop through each numerical column and create a box plot to visualize outliers\n",
        "for column in numerical_columns:\n",
        "    plt.figure()\n",
        "    sns.boxplot(x=merged_df[column])\n",
        "    plt.xlabel(column)\n",
        "    plt.title(f'Box Plot of {column} (with Outliers)')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boxplot is useful for displaying the distibution of a continuous variable and identifying outliers."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "bluzKgLaCfP3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detected outliers in 'cost', 'pictures', 'Reviewer_Total_Review', and 'Reviewer_Followers'. No outliers found in 'rating', 'year', 'month', and 'review hour'."
      ],
      "metadata": {
        "id": "O0umXql-CiGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in numerical_columns:\n",
        "    #plt.subplot(10, 4, 2)\n",
        "    plt.xlabel('Distribution of {}'.format(i))\n",
        "\n",
        "    # Calculate the IQR for the column\n",
        "    Q1 = merged_df[i].quantile(0.25)\n",
        "    Q3 = merged_df[i].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    # Define the upper and lower bounds for outlier detection\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Filter the data to remove outliers\n",
        "    filtered_data = merged_df[(merged_df[i] >= lower_bound) & (merged_df[i] <= upper_bound)]\n",
        "\n",
        "    # Plot the boxplot without outliers\n",
        "    sns.boxplot(x=i, data=filtered_data, color=\"tomato\")\n",
        "\n",
        "    #c += 1\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "f4XIFfmRI1kO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restricting the data to lower and upper boundary for cost in hotel dataset\n",
        "df1[(df1['Cost'] >= lower_bound) & (df1['Cost'] <= upper_bound)]"
      ],
      "metadata": {
        "id": "hU5ifACTI8wE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Restricting the data to lower and upper boundary for Reviewer followers in review dataset\n",
        "df2[(df2['Reviewer_Followers']>= lower_bound) & (df2['Reviewer_Followers'] <= upper_bound)]"
      ],
      "metadata": {
        "id": "DDqFrBpTI_hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What all outlier treatment techniques have you used and why did you use those techniques?\n",
        "\n",
        "The Interquartile Range (IQR) is a statistical measure representing the range of the middle 50% of the data. It summarizes data spread and is useful for handling outliers."
      ],
      "metadata": {
        "id": "kEUuuL_EJKNm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal of this code is to preprocess and transform categorical cuisine information from a DataFrame of restaurants into a format suitable for clustering by encoding the cuisines as separate binary columns."
      ],
      "metadata": {
        "id": "XhxIKJzWJgan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Encode your categorical columns\n",
        "\n",
        "# getting only necessary features for Clustering\n",
        "cluster_df = pd.merge(df1, price_point, how='right',\n",
        "                      on='Restaurant')[['Restaurant', 'Cost', 'Rating', 'Cuisines']]\n",
        "\n",
        "# Encoding the Cuisines as columns to further use in clustering\n",
        "\n",
        "#splitting cuisines into a list\n",
        "cluster_df['Cuisines'] = cluster_df['Cuisines'].apply(lambda x: x.split(', '))\n",
        "\n",
        "# adding the number of cuisines another feature\n",
        "cluster_df['no_of_cuisines'] = cluster_df['Cuisines'].apply(lambda x: len(x))\n",
        "\n",
        "\n",
        "# Create a list of all unique cuisines\n",
        "all_cuisines = list(set([cuisine for cuisines in cluster_df['Cuisines'] for cuisine in cuisines]))\n",
        "\n",
        "# Create a DataFrame with a column for each unique cuisine\n",
        "cuisine_df = pd.DataFrame(columns=all_cuisines)\n",
        "\n",
        "# Loop over each restaurant and create a feature vector\n",
        "for i, row in cluster_df.iterrows():\n",
        "    feature_vec = {cuisine: 0 for cuisine in all_cuisines}\n",
        "    for cuisine in row['Cuisines']:\n",
        "        feature_vec[cuisine] += 1\n",
        "    cuisine_df.loc[i] = feature_vec\n",
        "\n",
        "# Concatenate the original DataFrame with the cuisine DataFrame\n",
        "cluster_df = pd.concat([cluster_df, cuisine_df], axis=1)"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's make copy of cluster dataframe\n",
        "cluster_org_df = cluster_df.copy()"
      ],
      "metadata": {
        "id": "rrGAUO1aJvVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*  This code processes restaurant data for clustering. It extracts key\n",
        "features and encodes cuisines as columns. Cuisines are counted for each restaurant, forming a feature matrix. This matrix is combined with the original data for further analysis.\n",
        "*\n",
        "I used a technique similar to One Hot Encoding to represent cuisines as features with their counts for each restaurant. This enabled unsupervised clustering analysis."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Expand Contraction\n",
        "!pip install contractions"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating new df for text processing of sentiment analysis\n",
        "sentiment_df = df2[['Reviewer','Restaurant','Rating','Review']]\n",
        "#analysing two random sample\n",
        "sentiment_df.sample(2)"
      ],
      "metadata": {
        "id": "kO_4wy28OpjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#setting index\n",
        "sentiment_df = sentiment_df.reset_index()\n",
        "sentiment_df['index'] = sentiment_df.index"
      ],
      "metadata": {
        "id": "5d4ss1qWO3tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Lower Casing\n",
        "sentiment_df['Review'] = sentiment_df['Review'].str.lower()\n",
        "# random observation\n",
        "sentiment_df.sample(2)"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "import string\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    '''Removes punctuation marks from the given text.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): The text from which punctuation marks will be removed.\n",
        "\n",
        "    Returns:\n",
        "    str: The text stripped of punctuation marks.\n",
        "    '''\n",
        "    # Check if the input is NaN or None\n",
        "    if isinstance(text, str):\n",
        "        # Create a translator to remove punctuation marks\n",
        "        translator = str.maketrans('', '', string.punctuation)\n",
        "        # Return the text stripped of punctuation marks\n",
        "        return text.translate(translator)\n",
        "    else:\n",
        "        return text"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove punctuation using function created\n",
        "sentiment_df['Review'] = sentiment_df['Review'].apply(remove_punctuation)\n",
        "sentiment_df.sample(2)"
      ],
      "metadata": {
        "id": "mxWN49scPN3e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits\n",
        "import re\n",
        "\n",
        "def remove_non_letters(text):\n",
        "    '''Removes all non-letters from the given text.\n",
        "\n",
        "    Parameters:\n",
        "    text (str): The text from which non-letters will be removed.\n",
        "\n",
        "    Returns:\n",
        "    str: The text stripped of non-letters.\n",
        "    '''\n",
        "    # Use regex to remove all non-letter characters from the text\n",
        "    return re.sub(r'[^a-zA-Z]', ' ', text)"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace NaN values in the 'Review' column with an empty string\n",
        "sentiment_df['Review'] = sentiment_df['Review'].fillna('').apply(remove_non_letters)\n"
      ],
      "metadata": {
        "id": "fpzT0RS0PUrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to extract location of the restaurant\n",
        "def get_location(link):\n",
        "  link_elements = link.split(\"/\")\n",
        "  return link_elements[3]\n",
        "\n",
        "#create a location feature\n",
        "df1['Location'] = df1['Links'].apply(get_location)\n",
        "df1.sample(2)"
      ],
      "metadata": {
        "id": "FOdsShE6PX9q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "# extracting the stopwords from nltk library\n",
        "sw = stopwords.words('english')"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to remove stopwords\n",
        "def delete_stopwords(text):\n",
        "  '''a function for removing the stopword'''\n",
        "  # removing the stop words and lowercasing the selected words\n",
        "  text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
        "  # joining the list of words with space separator\n",
        "  return \" \".join(text)"
      ],
      "metadata": {
        "id": "DN7Adx3wPhVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#calling function to remove stopwords\n",
        "sentiment_df['Review'] = sentiment_df['Review'].apply(delete_stopwords)"
      ],
      "metadata": {
        "id": "mkyJQlyePj5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "sentiment_df['Review'] =sentiment_df['Review'].apply(lambda x: \" \".join(x.split()))"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random sample\n",
        "sentiment_df.sample(2)"
      ],
      "metadata": {
        "id": "DttROcWLPvMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is the process of splitting text into individual units, such as words or subwords, for further analysis in natural language processing."
      ],
      "metadata": {
        "id": "SZczHU4zQMYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Tokenization\n",
        "sentiment_df['Review'] = sentiment_df['Review'].apply(nltk.word_tokenize)"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "\n",
        "#applying Lemmatization\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Create a lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return lemmatized_tokens\n",
        "\n",
        "# Lemmatize the 'Review' column\n",
        "sentiment_df['Review'] = sentiment_df['Review'].apply(lemmatize_tokens)\n",
        "\n",
        "sentiment_df.sample(2)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization reduces words to their base form, considering context and grammar.\n",
        "\n",
        "It uses a dictionary-based approach for accurate results.\n",
        "Unlike stemming, it preserves context and grammatical correctness.\n",
        "\n",
        "Useful in NLP tasks for consistent and meaningful representations."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
        "vectorizer.fit(sentiment_df['Review'].values)\n",
        "#creating independent variable for sentiment analysis\n",
        "X_tfidf = vectorizer.transform(sentiment_df['Review'].values)"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Tf-idf Vectorization technique.\n",
        "\n",
        "TF-IDF (term frequency-inverse document frequency) is a technique that assigns a weight to each word in a document. It is calculated as the product of the term frequency (tf) and the inverse document frequency (idf)"
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping cuisine and restaurant from cluster_df\n",
        "cluster_df = cluster_df.drop(columns = ['Restaurant','Cuisines'], axis = 1)"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For sentiment analysis\n",
        "# Create a new column for sentiment based on the ratings\n",
        "def classify_sentiment(rating):\n",
        "    if rating >= 4:\n",
        "        return 'Positive'\n",
        "    elif rating >= 3:\n",
        "        return 'Neutral'\n",
        "    else:\n",
        "        return 'Negative'\n",
        "\n",
        "sentiment_df['Sentiment'] = sentiment_df['Rating'].apply(classify_sentiment)"
      ],
      "metadata": {
        "id": "g6cHcbvSUWlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First look of Sentiment Dataframe.\n",
        "sentiment_df.head()"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "\n",
        "# From analysis df, keeping only required features for final input\n",
        "sentiment_df = sentiment_df[['Restaurant', 'Review', 'Rating', 'Sentiment']]"
      ],
      "metadata": {
        "id": "kwLyzX9NUo00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Restaurant Clustering, we found that 'Cuisines', 'Cost', and 'Rating' were important features in performing the clustering analysis. Additionally, for Topic Modeling from Reviews data, we selected the text data and converted it to TF-IDF vectors."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature transformation, like log transforming right-skewed data, enhances machine learning model performance and interpretability, especially for skewed or non-normal data. Other numerical features exhibit symmetric data distribution."
      ],
      "metadata": {
        "id": "-ZVMr6LSWePV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "\n",
        "cluster_df['Cost'] = np.log(cluster_df['Cost']) # tranfomed Cost"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Select the numerical variables to standardize\n",
        "numerical_vars = ['Cost', 'Rating', 'no_of_cuisines']\n",
        "\n",
        "# Standardize the numerical variables using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "cluster_df[numerical_vars] = scaler.fit_transform(cluster_df[numerical_vars])"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Used Standard Scaler to address varying scales and ranges of numerical variables.\n",
        "2. Different scales can impact clustering results; e.g., a meal cost may overshadow a rating.\n",
        "3. Standardization ensures equal contribution of each variable to the clustering process.\n",
        "4. Results become more reliable and interpretable when variables are on the same scale."
      ],
      "metadata": {
        "id": "Xqa8LJyu7pUe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA()\n",
        "\n",
        "# Fit the PCA object to your standardized data\n",
        "pca.fit(cluster_df)\n",
        "\n",
        "# Get explained variance ratio of each principal component\n",
        "cumulative_var = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "# visualising the cummulative variance\n",
        "fig, ax = plt.subplots(figsize=(8,6))\n",
        "ax.plot(cumulative_var, marker='o', color='purple')\n",
        "ax.set_xlabel(\"Number of Components\")\n",
        "ax.set_ylabel(\"Cumulative Explained Variance\")\n",
        "ax.set_title(\"Cumulative Explained Variance vs Number of Components\")\n",
        "plt.xlim([0, 15])\n",
        "\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MrzewVeu763m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using n_component as 3\n",
        "pca = PCA(n_components=3)\n",
        "\n",
        "# fit PCA on features\n",
        "pca.fit(cluster_df)\n",
        "\n",
        "# transform data to principal component space\n",
        "pca_df = pca.transform(cluster_df)\n",
        "\n",
        "# variance explained by three components\n",
        "print(f'Cumulative variance explained by 3 principal components: {np.sum(pca.explained_variance_ratio_)}')"
      ],
      "metadata": {
        "id": "8GnWcKpN8gSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T64cbGGQ-u1k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have used Principal Component Analysis technique.\n",
        "\n",
        "PCA is commonly used for dimensionality reduction in unsupervised learning.\n",
        "Suitable for datasets with more than 40 features to simplify visualization and analysis.\n",
        "Reduces dimensionality by transforming original features into orthogonal principal components.\n",
        "Principal components are ordered by variance explained in the data.\n",
        "Helps retain most of the variation while reducing the complexity of the dataset."
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)\n",
        "\n",
        "In unsupervised learning, there is no target variable, so there is no concept of imbalance. The goal of unsupervised learning is to find patterns in the data without any reference to a known outcome. Therefore, imbalance is not a problem in unsupervised learning."
      ],
      "metadata": {
        "id": "Bm_chT1dC_32"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "# importing kmeans\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "#Within Cluster Sum of Squared Errors(WCSS) for different values of k\n",
        "wcss=[]\n",
        "for i in range(1,11):\n",
        "    km=KMeans(n_clusters=i,random_state = 20)\n",
        "    km.fit(pca_df)\n",
        "    wcss.append(km.inertia_)\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's generate an Elbow Curve to visualize the relationship between the number of clusters (K) and the WCSS\n",
        "plt.plot(range(1, 11), wcss, color='purple', marker='o', linestyle='-', linewidth=2)\n",
        "plt.xlabel(\"K Value\", size=20, color='purple')\n",
        "plt.xticks(np.arange(1, 11, 1))\n",
        "plt.ylabel(\"WCSS\", size=20, color='green')\n",
        "plt.title('Elbow Curve', size=20, color='blue')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xhpiK9NHEvP5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code:\n",
        "\n",
        "The \"Within-Cluster Sum of Squares\" (WCSS) values for different values of K (number of clusters) are plotted.\n",
        "The WCSS is calculated for K values ranging from 1 to 10.\n",
        "The plot displays the WCSS values on the y-axis and the corresponding K values on the x-axis.\n",
        "The plot represents an \"Elbow Curve,\" which helps to identify the optimal number of clusters based on the \"elbow point\" where the curve starts to flatten.\n",
        "The x-axis is labeled as \"K Value,\" and the y-axis is labeled as \"WCSS.\" The plot is styled with colors for better visualization."
      ],
      "metadata": {
        "id": "xBXovOyYE3SB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-means is a common clustering algorithm in machine learning that groups data based on similarities.\n",
        "It was applied to a restaurant dataset with cuisine, ratings, and cost to create clusters.\n",
        "Silhouette score is used to evaluate clustering quality, ranging from -1 to 1.\n",
        "The score of approximately 0.31 suggests somewhat distinct clusters.\n",
        "To interpret the silhouette visualizer graph:\n",
        "\n",
        "Look for a high average silhouette score, indicating well-separated and well-matched clusters.\n",
        "Seek even cluster sizes, represented by bars of similar width.\n",
        "Avoid overlapping bars, indicating suboptimal clustering.\n",
        "Ensure there are no negative silhouette scores."
      ],
      "metadata": {
        "id": "A2K7qebjF5-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's creates a scatter plot to visualize the clustering result\n",
        "plt.figure(figsize=(10, 6), dpi=120)\n",
        "\n",
        "kmeans = KMeans(n_clusters=5, init='k-means++', random_state=42)\n",
        "label = kmeans.fit_predict(pca_df)\n",
        "\n",
        "# Getting unique labels\n",
        "unique_labels = np.unique(label)\n",
        "\n",
        "# Plotting the results\n",
        "for i in unique_labels:\n",
        "    plt.scatter(pca_df[label == i, 0], pca_df[label == i, 1], label=f'Cluster {i+1}')\n",
        "\n",
        "plt.title('Visualization of Clusters')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*K-means and hierarchical clustering are methods to group data into clusters.\n",
        "\n",
        "*K-means uses centroids and means of data points, while hierarchical clustering uses similarity between data points.\n",
        "\n",
        "*Different criteria can cause different cluster labels for data points.\n",
        "\n",
        "*The number of clusters and initialization also affect the outcome."
      ],
      "metadata": {
        "id": "AHDwv_lDGLKq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dendrogram**\n",
        "\n",
        "A dendrogram is used to visualize hierarchical clustering relationships among data points. It helps in understanding how data forms clusters and subclusters based on similarities. It's valuable for selecting the number of clusters and understanding the data's inherent structure."
      ],
      "metadata": {
        "id": "b85QY-5oJv0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's generated Dendrogram using hierarchical clustering\n",
        "import scipy.cluster.hierarchy as sch\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "dendrogram = sch.dendrogram(sch.linkage(pca_df, method='ward'), orientation='top', labels=None,\n",
        "                           distance_sort='descending', show_leaf_counts=True)\n",
        "\n",
        "plt.title('Dendrogram')\n",
        "plt.xlabel('Restaurants')\n",
        "plt.ylabel('Euclidean Distances')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wAJgzxq3Jz7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dendrogram visually represents the distances (Euclidean) between restaurants, helping to identify potential clusters based on similarity."
      ],
      "metadata": {
        "id": "aZswZeBSJ8ch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Aglomerative Clustering**\n",
        "\n",
        "Agglomerative Clustering builds a hierarchy of clusters by repeatedly merging the closest ones, forming a dendrogram to show how data points group together."
      ],
      "metadata": {
        "id": "_IdO1hYzJ_u6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's visual representation of how the Silhouette score changes with varying numbers of clusters.\n",
        "\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "silhouette_scores = []\n",
        "range_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
        "\n",
        "for n_clusters in range_n_clusters:\n",
        "    hc = AgglomerativeClustering(n_clusters=n_clusters, affinity='euclidean', linkage='ward')\n",
        "    y_hc = hc.fit_predict(pca_df)\n",
        "    score = silhouette_score(pca_df, y_hc)\n",
        "    silhouette_scores.append(score)\n",
        "\n",
        "# Plot the silhouette scores against the number of clusters\n",
        "plt.plot(range_n_clusters, silhouette_scores, marker='o', linestyle='-', color='blue')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('Silhouette score')\n",
        "plt.title('Silhouette Score for Different Clusters')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9OVnlGTdKLjb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's creates a scatter plot to visualize the clustering result.\n",
        "from numpy import unique, where\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import davies_bouldin_score\n",
        "\n",
        "# Define the model\n",
        "model = AgglomerativeClustering(n_clusters=4)\n",
        "# Fit model and predict clusters\n",
        "y_hc = model.fit_predict(pca_df)\n",
        "# Retrieve unique clusters\n",
        "clusters = unique(y_hc)\n",
        "# Create scatter plot for samples from each cluster\n",
        "for cluster in clusters:\n",
        "    # Get row indexes for samples with this cluster\n",
        "    row_ix = where(y_hc == cluster)\n",
        "    # Create scatter of these samples\n",
        "    plt.scatter(pca_df[row_ix, 0], pca_df[row_ix, 1], label=f'Cluster {cluster+1}')\n",
        "\n",
        "# Show the plot\n",
        "plt.title('Agglomerative Clustering')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.legend()\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "# Evaluation\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Silhouette Coefficient\n",
        "silhouette = silhouette_score(pca_df, y_hc, metric='euclidean')\n",
        "print(\"Silhouette Coefficient: %0.3f\" % silhouette)\n",
        "\n",
        "# Davies Bouldin score of our clusters\n",
        "davies_bouldin = davies_bouldin_score(pca_df, y_hc)\n",
        "print(\"Davies Bouldin Score: %0.3f\" % davies_bouldin)\n"
      ],
      "metadata": {
        "id": "h9IV8HgrKP6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting the PCA data matrix into a Pandas Dataframe\n",
        "pca = pd.DataFrame(data=pca_df, columns=['pc1', 'pc2', 'pc3'])\n",
        "pca['labels'] = y_hc\n",
        "\n",
        "# Create scatter plot with larger and thicker data points\n",
        "plt.figure(figsize=(12, 8))\n",
        "scatter = plt.scatter(x=pca['pc1'], y=pca['pc2'], c=pca['labels'], s=50, cmap='cool', alpha=0.8)\n",
        "plt.title('Clustered Data Visualization')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "\n",
        "# Add legend\n",
        "handles, labels = scatter.legend_elements()\n",
        "legend = plt.legend(handles, labels, loc='best', title='Clusters')\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "5XsId4QXKV-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the 3D scatter plot using Plotly Express\n",
        "fig = px.scatter_3d(pca, x='pc1', y='pc2', z='pc3', color='labels',\n",
        "                    symbol='labels', size_max=10, opacity=0.8,\n",
        "                    title='Clustered Data Visualization 3D',\n",
        "                    labels={'labels': 'Clusters'})\n",
        "\n",
        "# Show the plot\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "oQ0BsmTQK-8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why did you pick the specific chart?\n",
        "\n",
        "3D scatter plot useful for it's visually represents clustered data, showing how data points are grouped into different clusters based on their principal components (pc1, pc2, and pc3).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jLEL7g6TNrGr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is/are the insight(s) found from the chart?\n",
        "\n",
        "Clustering algorithm has been successful in identifying meaningful groups of points."
      ],
      "metadata": {
        "id": "SY3k-zdaNxjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# adding cluster labels to the original restaurants data\n",
        "cluster_org_df = cluster_org_df[['Restaurant', 'Cost', 'Rating', 'Cuisines']]\n",
        "cluster_org_df['labels'] = y_hc"
      ],
      "metadata": {
        "id": "cjspyE5KNpx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cluster 1 - Street Flavor Hub"
      ],
      "metadata": {
        "id": "Jivzo3-TN2jP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this group, most of the dishes are fast food and local street food, which is why I named it \"Street Flavor Hub.\" This name emphasizes the cluster's focus on diverse and quick street-inspired delicacies."
      ],
      "metadata": {
        "id": "HomQwgDMN4Bs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's filters rows from Dataframe of cluster 1\n",
        "cluster_org_df[cluster_org_df['labels']==0]"
      ],
      "metadata": {
        "id": "231MCN47N6su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cluster 2 - Global Cuisine Delights\n"
      ],
      "metadata": {
        "id": "h5Yu28iqOOJv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cluster is named \"Global Cuisine Delights\" because it includes restaurants with a wide variety of cuisines from different parts of the world, like European, Mediterranean, Asian, and Italian.\n",
        "\n",
        "The name \"Global Cuisine Delights\" suggests that these restaurants offer delicious and enjoyable experiences with diverse international dishes.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l9qeYCnAORIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Let's filters rows from Dataframe of cluster 2\n",
        "cluster_org_df[cluster_org_df['labels']==1]"
      ],
      "metadata": {
        "id": "u4wQvNoPOXt-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cluster 3 - Asian Delights\n"
      ],
      "metadata": {
        "id": "Cc4tVEL4OdRq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cluster focuses on Asian cuisines like Chinese, Thai, and Momos, offering delicious and diverse Asian flavors.\n",
        "\n"
      ],
      "metadata": {
        "id": "4-mTPmbMOgZ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's filters rows from Dataframe of cluster 3\n",
        "cluster_org_df[cluster_org_df['labels']==2]"
      ],
      "metadata": {
        "id": "aujOeKJNOiyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cluster 4 - Snacks and Sweets Corner**\n"
      ],
      "metadata": {
        "id": "LTDE9DihOmVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this cluster, you can enjoy a variety of snacks and sweet treats, making it the perfect place for indulging in quick bites and desserts.\n",
        "\n"
      ],
      "metadata": {
        "id": "JQwghZjlOpgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's filters rows from Dataframe of cluster 4\n",
        "cluster_org_df[cluster_org_df['labels']==3]"
      ],
      "metadata": {
        "id": "COXfSgh_OrwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "KMeans Clustering:\n",
        "\n",
        "Applied K-means clustering to group restaurants based on given features.\n",
        "Used Elbow and Silhouette methods to determine the optimal number of clusters (n_clusters = 6).\n",
        "Fitted the model using K-means and labeled each data point with its cluster using 'KMeans.labels_'.\n",
        "Visualized the clusters and counted the number of restaurants in each cluster.\n",
        "Found that the majority of restaurants belonged to the first cluster.\n",
        "\n",
        "Agglomerative Hierarchical Clustering:\n",
        "\n",
        "Used Hierarchical Clustering - Agglomerative Model to cluster restaurants based on different features.\n",
        "Employed Silhouette Coefficient Score and used clusters = 4 for the model.\n",
        "Visualized the clusters and the data points within them."
      ],
      "metadata": {
        "id": "uzWZxEv1O6e-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topic Modeling\n",
        "\n",
        "Latent Dirchallent Allocation(LDA)"
      ],
      "metadata": {
        "id": "skgUpeOsPG8M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim.utils import simple_preprocess\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "random_seed = 42\n",
        "\n",
        "# Combine words in each review into a single string\n",
        "word_string = sentiment_df['Review'].apply(lambda words: ' '.join(words))\n",
        "\n",
        "# Tokenize the documents\n",
        "tokenized_docs = [simple_preprocess(doc) for doc in word_string]\n",
        "\n",
        "# Create a dictionary from the tokenized documents\n",
        "dictionary = corpora.Dictionary(tokenized_docs)\n",
        "\n",
        "# Convert the tokenized documents to a bag-of-words corpus\n",
        "bow_corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "\n",
        "# Train an LDA model on the bag-of-words corpus\n",
        "num_topics = 5  # The number of topics to extract\n",
        "lda_model = LdaModel(bow_corpus, num_topics=num_topics, id2word=dictionary, passes=10, random_state=random_seed)\n",
        "\n",
        "# Print the topics and their top 10 terms\n",
        "for topic in lda_model.show_topics(num_topics=num_topics, num_words=10, formatted=False):\n",
        "    print('Topic {}: {}'.format(topic[0], ', '.join([term[0] for term in topic[1]])))"
      ],
      "metadata": {
        "id": "-a30AL1VPKJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the complete topic-word distribution for the given LDA model\n",
        "topic_word_dist = lda_model.show_topics(num_topics=num_topics, num_words=10, formatted=False)\n",
        "\n",
        "# Visualize the top words for each topic using Plotly\n",
        "fig = go.Figure()\n",
        "\n",
        "num_topics_to_plot = 5\n",
        "\n",
        "for topic_idx, topic in topic_word_dist[:num_topics_to_plot]:\n",
        "    words = [word for word, _ in topic]\n",
        "    probabilities = [prob for _, prob in topic]\n",
        "    fig.add_trace(go.Bar(x=words, y=probabilities, name=f'Topic {topic_idx}', opacity=0.7))\n",
        "\n",
        "fig.update_layout(title='Top Words for Each Topic',\n",
        "                  xaxis_title='Top Words',\n",
        "                  yaxis_title='Probabilities',\n",
        "                  xaxis_tickangle=90,\n",
        "                  xaxis=dict(tickfont=dict(size=10, color='black')))\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "YXCFeqXcPazJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?\n",
        "\n",
        "Bar using Plotly is useful for visualizes the distribution of top words for each topic in a clear and interpretable manner."
      ],
      "metadata": {
        "id": "1IbEal10Pkzv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is/are the insight(s) found from the chart?\n",
        "Topic 0: Entertainment and Atmosphere\n",
        "\n",
        "Keywords: place, drink, one, food, night, time, music, floor, go, get\n",
        "\n",
        "This topic captures the essence of an entertainment venue with keywords related to nightlife, music, and going out for a good time.\n",
        "\n",
        "Topic 1: Food Varieties and Flavors\n",
        "\n",
        "Keywords: chicken, ordered, taste, good, rice, biryani, like, one, paneer, dish\n",
        "\n",
        "This topic focuses on various food items and flavors, particularly highlighting the enjoyment of chicken biryani and other dishes.\n",
        "\n",
        "Topic 2: Bad Food Experience\n",
        "\n",
        "Keywords: food, order, service, restaurant, time, bad, even, worst, dont, place\n",
        "\n",
        "This topic signifies negative experiences with keywords related to bad food, service, and overall dissatisfaction at a restaurant.\n",
        "\n",
        "Topic 3: Good Food and Service\n",
        "\n",
        "Keywords: good, food, place, service, ambience, great, taste, staff, starter, buffet\n",
        "\n",
        "This topic reflects positive dining experiences, highlighting good food, excellent service, and enjoyable ambiance.\n",
        "\n",
        "Topic 4: Best Restaurant in Town\n",
        "\n",
        "Keywords: place, food, good, best, nice, great, service, awesome, really, visit\n",
        "\n",
        "This topic suggests high praise for a top-tier restaurant, emphasizing its quality food, service, and overall excellence in the dining experience."
      ],
      "metadata": {
        "id": "PMf76lzJvfL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's Find perplextiy score for Topic 1 to 10.\n",
        "from gensim.models.ldamodel import LdaModel\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "# Assume that 'bow_corpus' is the bag-of-words corpus and 'dictionary' is the Dictionary object\n",
        "\n",
        "perplexity_scores = []\n",
        "\n",
        "for num_topics in range(1, 11):\n",
        "    lda_model = LdaModel(bow_corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
        "    perplexity = lda_model.log_perplexity(bow_corpus)\n",
        "    perplexity_scores.append(perplexity)\n",
        "    print(\"Number of Topics:\", num_topics, \"Perplexity:\", perplexity)\n",
        "\n",
        "print(\"Perplexity scores:\", perplexity_scores)"
      ],
      "metadata": {
        "id": "Cp0FWxXrwePi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's Find coherence score for Topic 1 to 10.\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim.corpora import Dictionary\n",
        "\n",
        "# Assume that 'bow_corpus' is the bag-of-words corpus and 'dictionary' is the Dictionary object\n",
        "\n",
        "coherence_scores = []\n",
        "\n",
        "for num_topics in range(1, 11):\n",
        "    lda_model = LdaModel(bow_corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
        "    coherence_model = CoherenceModel(model=lda_model, texts=tokenized_docs, dictionary=dictionary, coherence='c_v')\n",
        "    coherence_score = coherence_model.get_coherence()\n",
        "    coherence_scores.append(coherence_score)\n",
        "    print(\"Number of Topics:\", num_topics, \"Coherence Score:\", coherence_score)\n",
        "\n",
        "print(\"Coherence scores:\", coherence_scores)"
      ],
      "metadata": {
        "id": "TYL5MstiwhGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perplexity scores and coherence scores\n",
        "perplexity_scores = [-7.397, -7.363, -7.345, -7.355, -7.403, -7.400, -7.417, -7.452, -7.599, -7.742]\n",
        "coherence_scores = [0.386, 0.434, 0.399, 0.447, 0.465, 0.454, 0.477, 0.425, 0.440, 0.442]\n",
        "\n",
        "# Number of topics\n",
        "num_topics = range(1, 11)\n",
        "\n",
        "# Create a figure with two subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# Plotting perplexity scores\n",
        "ax1.plot(num_topics, perplexity_scores, marker='o')\n",
        "ax1.set_xlabel('Number of Topics')\n",
        "ax1.set_ylabel('Perplexity Score')\n",
        "ax1.set_title('Perplexity Scores vs. Number of Topics')\n",
        "ax1.set_xticks(num_topics)\n",
        "ax1.grid(True)\n",
        "\n",
        "# Plotting coherence scores\n",
        "ax2.plot(num_topics, coherence_scores, marker='o', color='r')\n",
        "ax2.set_xlabel('Number of Topics')\n",
        "ax2.set_ylabel('Coherence Score')\n",
        "ax2.set_title('Coherence Scores vs. Number of Topics')\n",
        "ax2.set_xticks(num_topics)\n",
        "ax2.grid(True)\n",
        "\n",
        "# Adjust layout for better visibility\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VGUXgNcJwkZd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perplexity scores are somewhat consistent across different numbers of topics, with only minor variations. On the other hand, the coherence scores seem to be peaking around 5 topics. Therefore, based on the given scores, I consider choosing 5 topics for my LDA model."
      ],
      "metadata": {
        "id": "0gA_g5L0wnFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis"
      ],
      "metadata": {
        "id": "pf-XJz1wwpw7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of reviews for each sentiment\n",
        "sentiment_counts = sentiment_df['Sentiment'].value_counts()\n",
        "\n",
        "# Create a pie chart to show the proportion of reviews for each sentiment\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "ax.pie(sentiment_counts, labels=sentiment_counts.index,\n",
        "       autopct='%1.1f%%', startangle=90, textprops={'fontsize': 15})\n",
        "ax.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "ax.set_title('Zomato Restaurant Review Sentiment Proportions')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y19HKZqewn7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?\n",
        "\n",
        "Count plot helps to find the count of dfferent kind of reviews distribution\n",
        "\n"
      ],
      "metadata": {
        "id": "nlDbgRvSw3KZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is/are the insight(s) found from the chart?\n",
        "\n",
        "The sentiment value counts reveal that most reviews are \"Positive\" (6268), followed by \"Negative\" (2448), and a smaller number are \"Neutral\" (1239). This suggests a generally positive sentiment overall, with a notable number of negative reviews as well."
      ],
      "metadata": {
        "id": "Ibm5UNn2w7NE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the words in the 'Review' list into a single string\n",
        "sentiment_df['ReviewText'] = sentiment_df['Review'].apply(lambda words: ' '.join(words))"
      ],
      "metadata": {
        "id": "1nVVsnUTxCDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform sentiment analysis using TextBlob\n",
        "def analyze_sentiment(text):\n",
        "    analysis = TextBlob(text)\n",
        "    return analysis.sentiment.polarity, analysis.sentiment.subjectivity\n",
        "\n",
        "sentiment_df[['Polarity', 'Subjectivity']] = sentiment_df['ReviewText'].apply(analyze_sentiment).apply(pd.Series)\n"
      ],
      "metadata": {
        "id": "TLRAPD8NxCvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create the scatter plot with vertical line\n",
        "fig = px.scatter(sentiment_df,\n",
        "                 x='Polarity',\n",
        "                 y='Subjectivity',\n",
        "                 color='Sentiment',\n",
        "                 size='Subjectivity',\n",
        "                 hover_name='ReviewText',\n",
        "                 hover_data=['Rating'],\n",
        "                 title='Sentiment Analysis')\n",
        "\n",
        "# Add a vertical line at x=0 for Neutral Reviews\n",
        "fig.update_layout(shapes=[dict(type='line',\n",
        "                               yref='paper', y0=0, y1=1,\n",
        "                               xref='x', x0=0, x1=0)])\n",
        "\n",
        "# Show the plot\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "Vxbj-Q6MxJkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why did you pick the specific chart?\n",
        "\n",
        "\"Sentiment Analysis\" is useful for displays the sentiment polarity (positive or negative) of reviews.\n",
        "\n",
        "We use this plot to quickly identify and analyze the distribution of sentiments in the reviews, making it helpful for understanding customer feedback, product perception, and overall sentiment patterns.\n",
        "\n"
      ],
      "metadata": {
        "id": "33YlM2krxOoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is/are the insight(s) found from the chart?\n",
        "\n",
        "The histogram displays sentiment polarity distribution: bars on the left signify negative sentiment, while those on the right signify positive sentiment\n",
        "\n",
        "The plot allows for a quick and easy visualization of the sentiment polarity distribution of the reviews, helping to understand the overall sentiment of customers towards the restaurants."
      ],
      "metadata": {
        "id": "XjbHFXRUxVeQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Enhancing Zomato Restaurant Experience through Negative Word Analysis"
      ],
      "metadata": {
        "id": "N0odiniDxZ6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Function to extract negative words from a list of words\n",
        "def extract_negative_words(review):\n",
        "    negative_words_in_review = [word.lower() for word in review if sia.polarity_scores(word)['compound'] < 0]\n",
        "    return negative_words_in_review\n"
      ],
      "metadata": {
        "id": "g4HhqkcnxbBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the function to the 'Review' column and create a new column 'NegativeWords'\n",
        "sentiment_df['NegativeWords'] = sentiment_df['Review'].apply(extract_negative_words)\n",
        "\n",
        "\n",
        "# Create a dictionary to store the negative words for each restaurant\n",
        "restaurant_negative_words = {}\n",
        "\n",
        "for index, row in sentiment_df.iterrows():\n",
        "    restaurant = row['Restaurant']\n",
        "    negative_words = row['NegativeWords']\n",
        "\n",
        "    if restaurant in restaurant_negative_words:\n",
        "        restaurant_negative_words[restaurant].extend(negative_words)\n",
        "    else:\n",
        "        restaurant_negative_words[restaurant] = negative_words"
      ],
      "metadata": {
        "id": "U2wCk8p5xiPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the dictionary to a DataFrame\n",
        "df_negative_words = pd.DataFrame(list(restaurant_negative_words.items()), columns=['Restaurant', 'NegativeWords'])\n",
        "\n",
        "df_negative_words.head(5)\n"
      ],
      "metadata": {
        "id": "sjuXZ0RsxmLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart 1: WordCloud of Negative Words\n",
        "lists_of_words = df_negative_words.head(15)['NegativeWords'].tolist()\n",
        "all_words = [word for sublist in lists_of_words for word in sublist]\n",
        "negative_words_text = ' '.join(all_words)\n",
        "\n",
        "wordcloud = WordCloud(width=400, height=400, background_color='black').generate(negative_words_text)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Subplot 1: WordCloud\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud of Negative Words from Restaurants')\n",
        "\n",
        "# Chart 2: Bar Chart of Top 15 Negative Words\n",
        "all_negative_words = [word for sublist in df_negative_words['NegativeWords'] for word in sublist]\n",
        "word_counts = Counter(all_negative_words)\n",
        "top_words = word_counts.most_common(15)\n",
        "word_list, frequency_list = zip(*top_words)\n",
        "top_words_df = pd.DataFrame({'Words': word_list, 'Frequencies': frequency_list})\n",
        "\n",
        "# Subplot 2: Bar Chart\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.barplot(x='Frequencies',\n",
        "            y='Words',\n",
        "            data=top_words_df,\n",
        "            palette='Reds',\n",
        "            order=top_words_df.sort_values('Frequencies', ascending=True).Words)\n",
        "\n",
        "plt.title('Top 15 Most Frequent Negative Words in Reviews', fontsize=15)\n",
        "plt.xlabel('Frequency', fontsize=12)\n",
        "plt.ylabel('Words', fontsize=12)\n",
        "plt.gca().invert_yaxis()\n",
        "\n",
        "# Adjust layout and show the plot\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ouabAfJRxqZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Negative Restaurant Aspects:\n",
        "\n",
        "Taste: Words such as \"bad\" and \"pathetic\" are indicative of negative sentiments towards the taste of the food.\n",
        "\n",
        "Price: The use of words like \"waste,\" \"hard,\" and \"pay\" suggests dissatisfaction with the pricing or value for money.\n",
        "\n",
        "Behavior: Terms like \"worst,\" \"shake,\" and \"rude\" portray negative experiences related to staff behavior.\n",
        "\n",
        "Hygiene: The term \"horrible\" points to concerns about hygiene or cleanliness.\n",
        "\n",
        "Quality: The word \"poor\" highlights a negative perception of the food quality.\n",
        "\n",
        "Service: Negative sentiments about service are conveyed through words like \"wrong\" and \"miss.\"\n",
        "\n",
        "Experience: Words like \"disappointed\" and \"disappointing\" reflect an overall negative dining experience.\n",
        "\n",
        "Other: The term \"limited\" implies dissatisfaction with the variety or options available."
      ],
      "metadata": {
        "id": "yaLwnssHxwD-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive Word Analysis**"
      ],
      "metadata": {
        "id": "Mpr_yuXmx6K7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter reviews with rating more than 3\n",
        "positive_reviews = sentiment_df[sentiment_df['Rating'] > 3]['ReviewText']\n",
        "\n",
        "# Combine all positive reviews into a single string\n",
        "positive_text = ' '.join(positive_reviews)\n",
        "\n",
        "# Create a WordCloud object\n",
        "wordcloud = WordCloud(width=400, height=400,\n",
        "                      background_color='black',\n",
        "                      min_font_size=10).generate(positive_text)\n",
        "\n",
        "# Tokenize the text into individual words\n",
        "words = positive_text.split()\n",
        "\n",
        "# Count the frequency of each word\n",
        "word_freq = Counter(words)\n",
        "\n",
        "# Get the top 15 most frequent words and their frequencies\n",
        "top_words = word_freq.most_common(15)\n",
        "top_words, frequencies = zip(*top_words)\n",
        "\n",
        "# Create a DataFrame for the top words and their frequencies\n",
        "top_words_df = pd.DataFrame({'Words': top_words, 'Frequencies': frequencies})\n",
        "\n",
        "# Create subplots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Plot WordCloud on the left subplot\n",
        "axes[0].imshow(wordcloud)\n",
        "axes[0].axis('off')\n",
        "axes[0].set_title('WordCloud of Positive Reviews')\n",
        "\n",
        "# Plot bar plot on the right subplot\n",
        "sns.barplot(x='Frequencies', y='Words', data=top_words_df,\n",
        "            order=top_words_df.sort_values('Frequencies', ascending=True).Words,\n",
        "            ax=axes[1])\n",
        "axes[1].set_title('Top 15 Most Frequent Words in Positive Reviews')\n",
        "axes[1].set_xlabel('Frequency')\n",
        "axes[1].set_ylabel('Words')\n",
        "axes[1].invert_yaxis()\n",
        "\n",
        "# Adjust layout and display\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EA0QEHCux3g6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Positive Restaurant Aspects:\n",
        "\n",
        "Food Quality: Words like \"good,\" \"great,\" and \"taste\" indicate positive sentiments towards the food's taste and quality.\n",
        "\n",
        "Service and Ambience: \"Service\" and \"ambience\" are noted, suggesting positive customer experiences in terms of service and atmosphere.\n",
        "\n",
        "Variety: The mention of specific food items like \"chicken\" and \"try\" points to a diverse menu offering.\n",
        "\n",
        "Overall Experience: Words like \"nice,\" \"best,\" and \"really\" reflect positive overall dining experiences.\n",
        "\n",
        "These insights highlight that customers appreciate the quality of food, service, ambience, and diverse offerings in the restaurants, leading to positive sentiments in their reviews."
      ],
      "metadata": {
        "id": "-JdDCRhHyEBR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "TMVrXyFJyOg-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Insights: This project uncovered valuable insights through comprehensive data analysis, providing meaningful recommendations for Zomato and restaurant owners.\n",
        "\n",
        "Engagement and Ratings: AB's - Absolute Barbecues emerged as a highly engaged restaurant with the highest average ratings, while Hotel Zara Hi-Fi faced lower engagement and ratings.\n",
        "\n",
        "Price-Performance Correlation: Restaurants like AB's - Absolute Barbecues showcased a positive correlation between higher prices and better ratings, whereas Hotel Zara Hi-Fi displayed lower ratings with a lower price point.\n",
        "\n",
        "Cuisines Demand: North Indian and Chinese cuisines were top favorites among customers, offered by most restaurants, indicating their widespread popularity.\n",
        "\n",
        "Tag Trends: The tag \"Great Buffets\" stood out as frequently used, along with tags like \"great,\" \"best,\" \"north,\" and \"Hyderabad,\" providing insights into customer sentiments.\n",
        "\n",
        "Critic Impact: Critic Satwinder Singh's influence was notable with a substantial follower count and an average rating of 3.5.\n",
        "\n",
        "Price and Ratings: Collage - Hyatt Hyderabad Gachibowli, as the most expensive restaurant, maintained a 3.5 average rating. Comparatively, affordable options like Amul and Mohammedia Shawarma retained higher ratings.\n",
        "\n",
        "Cluster Profiling: Clustering revealed distinct restaurant clusters like Street Flavor Hub, Global Cuisine Delights, Asian Delights, and Snacks and Sweets Corner.\n",
        "\n",
        "Sentiment Analysis: Integrating sentiment analysis added a layer of customer feedback interpretation, contributing to a holistic understanding of customer opinions and sentiments.\n",
        "\n",
        "Future Prospects: Potential future work involves developing personalized recommendation systems based on user preferences and reviews to enhance customer experiences.\n",
        "\n",
        "In conclusion, this project effectively harnessed data-driven insights, including sentiment analysis, to empower Zomato and restaurant owners with informed decisions and enhanced customer satisfaction."
      ],
      "metadata": {
        "id": "HzJLLKlryQxE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "THANK YOU !!!"
      ],
      "metadata": {
        "id": "cI4FypkPyWy_"
      }
    }
  ]
}